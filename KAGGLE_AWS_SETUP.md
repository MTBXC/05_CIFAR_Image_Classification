# ğŸ” AWS Credentials Setup for Kaggle Notebooks

This guide explains how to create and configure AWS credentials so your Kaggle notebooks can save MLFlow artifacts to your S3 bucket.

## ğŸ¤” Why do I need this?

Your MLFlow server on EC2 uses **IAM Role** to access S3 (no keys needed). However, Kaggle notebooks run **outside AWS** and need **Access Keys** to authenticate.

## ğŸ“‹ Current Setup

- **S3 Bucket**: `mlflow-minimal-z01phmk8`
- **AWS Region**: `eu-north-1`
- **MLFlow Server**: `http://13.51.104.28:5000`

## ğŸš€ Step-by-Step Instructions

### Step 1: Create IAM User with Access Keys

Navigate to the Terraform directory and apply the new configuration:

```bash
cd infra/aws/terraform-minimal

# Apply the new IAM user configuration
terraform apply
```

This will create:
- IAM User: `mlflow-minimal-kaggle-user`
- Access Key ID and Secret Access Key
- S3 permissions (same as EC2 role)

### Step 2: Get Your Credentials

After `terraform apply`, get your credentials:

```bash
# Get Access Key ID
terraform output kaggle_aws_access_key_id

# Get Secret Access Key (sensitive!)
terraform output -raw kaggle_aws_secret_access_key
```

**âš ï¸ IMPORTANT:** Copy these values immediately! The Secret Access Key is only shown once.

### Step 3: Add Credentials to Kaggle Secrets

1. **Open your Kaggle Notebook**
   - Go to: https://www.kaggle.com/code

2. **Access Secrets Menu**
   - Click **"Add-ons"** (right sidebar)
   - Select **"Secrets"**

3. **Add AWS_ACCESS_KEY_ID**
   - Click **"Add a new secret"**
   - Label: `AWS_ACCESS_KEY_ID`
   - Value: Paste the Access Key ID from Step 2
   - Click **"Add"**

4. **Add AWS_SECRET_ACCESS_KEY**
   - Click **"Add a new secret"** again
   - Label: `AWS_SECRET_ACCESS_KEY`
   - Value: Paste the Secret Access Key from Step 2
   - Click **"Add"**

5. **Enable Secrets in Your Notebook**
   - Toggle **ON** for both secrets
   - You should see: ğŸ”“ AWS_ACCESS_KEY_ID (Enabled)
   - You should see: ğŸ”“ AWS_SECRET_ACCESS_KEY (Enabled)

### Step 4: Use Credentials in Kaggle Notebook

Add this **Cell 0** to your Kaggle notebook (before Cell 1):

```python
# ====================================================================
# ğŸ” Cell 0: AWS S3 Configuration for MLFlow Artifacts
# ====================================================================

import os

# Load AWS credentials from Kaggle Secrets
try:
    from kaggle_secrets import UserSecretsClient
    user_secrets = UserSecretsClient()
    
    os.environ['AWS_ACCESS_KEY_ID'] = user_secrets.get_secret("AWS_ACCESS_KEY_ID")
    os.environ['AWS_SECRET_ACCESS_KEY'] = user_secrets.get_secret("AWS_SECRET_ACCESS_KEY")
    os.environ['AWS_DEFAULT_REGION'] = 'eu-north-1'  # Your MLFlow S3 region
    
    print("âœ… AWS credentials loaded from Kaggle Secrets")
    print(f"âœ… Region: {os.environ['AWS_DEFAULT_REGION']}")
    print("âœ… MLFlow can now save artifacts to S3!")
    
except Exception as e:
    print(f"âŒ Failed to load AWS credentials: {e}")
    print("âš ï¸  MLFlow will work, but artifacts may not be saved to S3")
```

## ğŸ§ª Test the Configuration

Test if credentials work:

```python
import boto3

try:
    s3 = boto3.client('s3')
    response = s3.list_objects_v2(Bucket='mlflow-minimal-z01phmk8', MaxKeys=1)
    print("âœ… S3 connection successful!")
except Exception as e:
    print(f"âŒ S3 connection failed: {e}")
```

## ğŸ“Š Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Your Setup                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   Kaggle    â”‚         â”‚   EC2 Server â”‚                 â”‚
â”‚  â”‚  Notebook   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   (MLFlow)   â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  HTTP   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚        â”‚                        â”‚                          â”‚
â”‚        â”‚ AWS Access Keys        â”‚ IAM Role                â”‚
â”‚        â”‚                        â”‚                          â”‚
â”‚        â–¼                        â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚          S3 Bucket                  â”‚                  â”‚
â”‚  â”‚   mlflow-minimal-z01phmk8           â”‚                  â”‚
â”‚  â”‚   (MLFlow Artifacts Storage)        â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Flow:
1. Kaggle sends logs/metrics to MLFlow Server via HTTP
2. MLFlow Server stores artifacts in S3 using IAM Role
3. Kaggle can also directly upload to S3 using Access Keys
```

## ğŸ”’ Security Best Practices

1. **Never commit credentials to git**
   - The `.gitignore` already excludes terraform state files
   - Never hardcode keys in notebooks

2. **Use Kaggle Secrets** (as shown above)
   - Don't print credentials in notebook output
   - Don't share notebooks with secrets enabled

3. **Rotate keys periodically**
   ```bash
   # Destroy old key and create new one
   terraform apply -replace="aws_iam_access_key.kaggle_mlflow"
   ```

4. **Limit permissions**
   - The IAM user only has access to the MLFlow S3 bucket
   - No other AWS resources can be accessed

## ğŸ—‘ï¸ Cleanup

If you no longer need Kaggle access:

```bash
cd infra/aws/terraform-minimal

# Remove the IAM user and keys
rm iam_user_for_kaggle.tf
terraform apply
```

## â“ Troubleshooting

### Problem: "No module named 'kaggle_secrets'"
**Solution:** You're running locally, not in Kaggle. Use direct environment variables or skip AWS config.

### Problem: "Access Denied" when accessing S3
**Solution:** 
1. Check if secrets are enabled in Kaggle
2. Verify credentials with: `terraform output kaggle_aws_access_key_id`
3. Ensure region is correct: `eu-north-1`

### Problem: "Bucket does not exist"
**Solution:** Get bucket name: `terraform output kaggle_s3_bucket_name`

### Problem: MLFlow works but artifacts not in S3
**Solution:** This is normal if AWS credentials are not configured. Artifacts will be stored locally on MLFlow server.

## ğŸ“š Additional Resources

- [Kaggle Secrets Documentation](https://www.kaggle.com/docs/notebooks#secrets)
- [AWS IAM Best Practices](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html)
- [MLFlow S3 Artifact Store](https://mlflow.org/docs/latest/tracking.html#amazon-s3-and-s3-compatible-storage)

## âœ… Summary

After following these steps:
- âœ… IAM User created with S3 access
- âœ… Access Keys stored in Kaggle Secrets
- âœ… Kaggle notebooks can log to MLFlow
- âœ… Artifacts automatically saved to S3
- âœ… Secure and production-ready setup






